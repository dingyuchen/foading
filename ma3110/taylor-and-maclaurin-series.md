# Taylor and Maclaurin Series

We say that a function $f$ is **infinitely differentiable** on $(a, b)$ if $f^{(n)}(x)$ exists for all $x \in (a, b)$ and for all $n \in \N$.

Let $f$ be infinitely differentiable on $(x_0 - r, x_0 + r)$ for some $r > 0$.
The power series

$$\sum^\infty_{n = 0} \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n$$

is called the **Taylor Series of $f$ about $x_0$**.

If $x_0 = 0$, the series becomes 

$$\sum^\infty_{n = 0} \frac{f^{(n)}(0)}{n!} x^n$$

called the **Maclaurin Series**.

## Theorem 9.3.1

Let $f$ be infinitely differentiable on $I = (x_0 - r, x_0 + r)$ and $x \in I$.
Then

$$f(x) = \sum^\infty_{n = 0} \frac{f^{(n)}(x_0)}{n!} (x - x_0)^n$$

iff 

$$\lim_{n \rightarrow \infty} R_n(x) = \lim_{n \rightarrow \infty} \frac{f^{(n+1)}(c_n)}{(n + 1)!} (x - x_0)^{n+1} = 0$$

where each $c_n$ (depending on $n, x, x_0$) is between $x$ and $x_0$.

## Analytic

A function $f$ is **analytic** on $(a, b)$ if 

1. $f$ is infinitely differentiable on $(a, b)$ and
2. for any $x_0 \in (a, b)$, the Taylor Series of $f$ about $x_0$ converges to $f$ in a neighborhood of $x_0$.

## Arithmetic Operations with Power Series

The **Cauchy product** of $\sum^\infty_{n = 0} a_n$ and $\sum^\infty_{n = 0} b_n$ is the series $\sum^\infty_{n = 0} c_n$, where for each $n \in \N$,

$$c_n = \sum^n_{k = 0} a_k b_{n-k} = a_0b_n + a_1b_{n - 1} + ... + a_n b_0 $$

> $c_n$ is the sum of all products where $i + j = n$.

## Merten's Theorem

If $\sum^\infty_{n = 0} a_n$ converges absolutely and $\sum^\infty_{n = 0} b_n$ converges, then the Cauchy product 
$\sum^\infty_{n = 0} c_n$ of these 2 series converges and 

$$\sum^\infty_{n = 0} c_n = \bigg ( \sum^\infty_{n = 0} b_n \bigg) \bigg(\sum^\infty_{n = 0} a_n \bigg )$$

## Theorem 9.4.2

Let

$$f(x) = \sum^\infty_{n = 0} a_n(x - x_0)^n, \vert x - x_0 \vert < R_1$$

$$g(x) = \sum^\infty_{n = 0} b_n(x - x_0)^n, \vert x - x_0 \vert < R_2$$

and $\alpha, \beta$ are constants. Then

1. $\alpha f(x) + \beta g(x) = \sum (\alpha a_n + \beta b_n)(x - x_0)^n$ for $\vert x - x_0 \vert < \min(R_1, R_2)$ and
2. $f(x) g(x) = \sum c_n(x - x_0)^n, \vert x - x_0 \vert < \min(R_1, R_2)$, where $c_n = \sum^n_{k = 0} a_k b_{n - k}$ (the nth term of the Cauchy product of $\sum a_n$ and $\sum b_n$)

### Proof

By Merten's Theorem, their Cauchy product converges absolutely to $f(x) g(x)$.

The nth term of the Cauchy product is 

$$\sum^n_{k = 0} a_k (x - x_0)^k b_{n - k}(x - x_0)^{n - k} = \sum^n_{k = 0} a_k b_{n - k} (x - x_0)^n = c_n(x - x_0)^n$$

## Redefining:

- [[exponential-function]]
- [[logarithmic-function]]

## Corollary 9.6.2

Let $\alpha \in \R$.
Then the function $f: (0, \infty) \rightarrow \R$ defined by

$$f(x) = x^\alpha$$

is differentiable on $(0, \infty)$ and 

$$f'(x) = \alpha x^{\alpha - 1}$$

> Extend formula on rationals to all real numbers by using $x^a = e^{a \ln x}$.

## Trigonometric Functions

- [[cosine-function]]
- [[sine-function]]

### Theorem 9.7.1

We have $C'(x) = -S(x)$ and $S'(x) = C(x)$.


[//begin]: # "Autogenerated link references for markdown compatibility"
[exponential-function]: exponential-function "Exponential Function"
[logarithmic-function]: logarithmic-function "Logarithmic Function"
[cosine-function]: cosine-function "Cosine Function"
[sine-function]: sine-function "Sine Function"
[//end]: # "Autogenerated link references"